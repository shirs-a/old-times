---
title: k8s网络   
date: 2024-04-18 16:22:10
permalink: /pages/8309a5b876fc9020
categories: 
  - k8s
tags: 
  - null
author: 
  name: shirongsheng
  link: https://github.com/shirongsheng
---

## 为什么要用kube-proxy

容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，***一旦Pod变化，则该Pod提供的服务也就无法访问***，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。

解决这个问题的办法和传统数据中心解决无状态服务高可用的思路完全一样，通过 ***负载均衡和VIP*** 实现后端真实服务的自动转发、故障转移.

***这个负载均衡在Kubernetes中称为Service，VIP即Service ClusterIP***，因此可以认为Kubernetes的Service就是一个四层负载均衡，Kubernetes对应的还有七层负载均衡Ingress.

这个Service就是由kube-proxy实现的，ClusterIP不会因为Pod状态改变而变，***需要注意的是VIP即ClusterIP是个假的IP，这个IP在整个集群中根本不存在，当然也就无法通过IP协议栈无法路由，底层underlay设备更无法感知这个IP的存在，因此ClusterIP只能是单主机(Host Only）作用域可见，这个IP在其他节点以及集群外均无法访问。***

Kubernetes为了实现在集群所有的节点都能够访问Service，***kube-proxy默认会在所有的Node节点都创建这个VIP并且实现负载，所以在部署Kubernetes后发现kube-proxy是一个DaemonSet。***

而Service负载之所以能够在Node节点上实现是因为无论Kubernetes使用哪个网络模型，均需要保证满足如下三个条件：

- 容器之间要求不需要任何NAT能直接通信；
- 容器与Node之间要求不需要任何NAT能直接通信；
- 容器看到自身的IP和外面看到它的IP必须是一样的，即不存在IP转化的问题。
至少第2点是必须满足的，有了如上几个假设，Kubernetes Service才能在Node上实现，否则Node不通Pod IP也就实现不了了。

有人说既然kube-proxy是四层负载均衡，那kube-proxy应该可以使用haproxy、nginx等作为负载后端啊？

事实上确实没有问题，不过唯一需要考虑的就是性能问题，如上这些负载均衡功能都强大，但毕竟还是基于用户态转发或者反向代理实现的，性能必然不如在内核态直接转发处理好。

因此kube-proxy默认会优先选择基于内核态的负载作为后端实现机制，目前kube-proxy默认是通过iptables实现负载的，在此之前还有一种称为userspace模式，其实也是基于iptables实现，可以认为当前的iptables模式是对之前userspace模式的优化。

## 负载均衡

<img src="/old-times/png/k8s/net.png">

### 二层负载均衡(mac)

根据 OSI 模型分的二层负载，一般是用虚拟 MAC 地址方式，外部对虚拟 MAC 地址请求，负载均衡接收后分配后端实际的 MAC 地址响应.

### 三层负载均衡

一般采用虚拟 IP 地址方式，外部对虚拟的 IP 地址请求，负载均衡接收后分配后端实际的 IP 地址响应。

### 四层负载均衡

在三层负载均衡的基础上，用 ip+port 接收请求，再转发到对应的机器。

### 七层负载均衡

根据虚拟的 url 或 IP，主机名接收请求，再转向相应的处理服务器。

## iptables

### 四表

iptables的四个表iptable_filter，iptable_mangle，iptable_nat，iptable_raw，默认表是filter（没有指定表的时候就是filter表）。

- filter 表：用来对数据包进行过滤，具体的规则要求决定如何处理一个数据包。
对应的内核模块为：iptable_filter，其表内包括三个链：input、forward、output;
- nat 表：nat 全称：network address translation 网络地址转换，主要用来修改数据包的 IP 地址、端口号信息。
对应的内核模块为：iptable_nat，其表内包括三个链：prerouting、postrouting、output;
- mangle 表：主要用来修改数据包的服务类型，生存周期，为数据包设置标记，实现流量整形、策略路由等。
对应的内核模块为：iptable_mangle，其表内包括五个链：prerouting、postrouting、input、output、forward;
- raw 表：主要用来决定是否对数据包进行状态跟踪。
对应的内核模块为：iptable_raw，其表内包括两个链：output、prerouting;
***raw表只使用在PREROUTING链和OUTPUT链上,因为优先级最高，从而可以对收到的数据包在系统进行ip_conntrack（连接跟踪）前进行处理。一但用户使用了raw表,在某个链上，raw表处理完后，将跳过NAT表和ip_conntrack处理，即不再做地址转换和数据包的链接跟踪处理了。RAW表可以应用在那些不需要做nat的情况下，以提高性能。***

### 五链

iptables的五个链PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING。

- input 链：当收到访问防火墙本机地址的数据包时，将应用此链中的规则；
- output 链：当防火墙本机向外发送数据包时，将应用此链中的规则；
- forward 链：当收到需要通过防火中转发给其他地址的数据包时，将应用此链中的规则，注意如果需要实现forward转发需要开启Linux内核中的ip_forward功能；
- prerouting 链：在对数据包做路由选择之前，将应用此链中的规则；
- postrouting 链：在对数据包做路由选择之后，将应用此链中的规则；

## kube-proxy使用iptables存在的问题

基于iptables模式的kube-proxy ClusterIP和NodePort都是基于iptables规则实现的，我们至少发现存在如下几个问题：

- iptables规则复杂零乱，真要出现什么问题，排查iptables规则必然得掉层皮。LOG + TRACE 大法也不好使。
- iptables规则多了之后性能下降，这是因为iptables规则是基于链表实现，查找复杂度为O(n)，当规模非常大时，查找和处理的开销就特别大。据官方说法，当节点到达5000个时，假设有2000个NodePort Service，每个Service有10个Pod，那么在每个Node节点中至少有20000条规则，内核根本支撑不住，iptables将成为最主要的性能瓶颈。
- iptables主要是专门用来做主机防火墙的，而不是专长做负载均衡的。虽然通过iptables的statistic模块以及DNAT能够实现最简单的只支持概率轮询的负载均衡，但是往往我们还需要更多更灵活的算法，比如基于最少连接算法、源地址HASH算法等。而同样基于netfilter的ipvs却是专门做负载均衡的，配置简单，基于散列查找O(1)复杂度性能好，支持数十种调度算法。因此显然ipvs比iptables更适合做kube-proxy的后端，毕竟专业的人做专业的事，物尽其美。

## ipvs(k8s1.11版本正式引入)

***ipvs没有像iptables那样存在各种table，table嵌套各种链，链里串着一堆规则，ipvsadm就只有两个核心实体，分别为service和server，service就是一个负载均衡实例，而server就是后端member,ipvs术语中叫做real server，简称RS。***

在k8s中service就是clusterIp,RS就是Pod ip.

### 用法

ipvcadm  
- -L|l  列举  list the table
- -A  添加 add virtual service with option
- -D  删除 delete virtual service
- -S  查看保存的规则 save rules to stdout
- -n  以数字形式展现 numeric output of addresses and ports


### k8s中使用ipvs

查看配置

```
kubectl get cm -n kube-system kube-proxy -o yaml

可以看到如下配置:

mode: ipvs  //选用ipvs模式

ipvs:
  scheduler: rr  //轮询调度算法 round robin

```

#### service clusterip原理

***由于下面这个配置，我们无论在节点上还是容器内去curl 10.100.165.36:6565，都是可以通的，因为最终都会转发到Pod所在的地址，前提是容器内端口是正确的***

```
# kubectl get svc -n shirs
shirs-v1   ClusterIP   10.100.165.36   <none>        6565/TCP   2m11s

查看ipvs配置如下

# ipvsadm -S -n | grep 10.100.165.36
-A -t 10.100.165.36:6565 -s rr
-a -t 10.100.165.36:6565 -r 10.240.3.26:14707 -m -w 1
-a -t 10.100.165.36:6565 -r 10.240.3.26:14707 -m -w 1

```

可见ipvs的LB IP为ClusterIP，算法为rr，RS为Pod的IP，我有两个Pod实例        

***其中-t指定service实例，-r指定server地址，-w指定权值，-m即前面说的转发模式，其中-m表示为masquerading，即NAT模式，-g为gatewaying，即直连路由模式，-i为ipip,即IPIP隧道模式。***

***发现使用的模式为NAT模式，这是显然的，因为除了NAT模式支持端口映射，其他两种均不支持端口映射，所以必须选择NAT模式。***

```
ipvs的VIP必须在本地存在,可通过ip addr show kube-ipvs0查看

# ip addr show kube-ipvs0
4: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default
    link/ether 46:6b:9e:af:b0:60 brd ff:ff:ff:ff:ff:ff
    inet 10.100.165.36/32  scope global kube-ipvs0
       valid_lft forever preferred_lft forever
```

kube-proxy首先会创建一个dummy虚拟网卡kube-ipvs0，然后把所有的Service IP添加到kube-ipvs0中。

基于iptables的Service，ClusterIP是一个虚拟的IP，因此这个IP是ping不通的，但ipvs中这个IP是在每个节点上真实存在的，因此可以ping通.

### 总结

Kubernetes的ClusterIP和NodePort都是通过ipvs service实现的，Pod当作ipvs service的server，通过NAT MQSQ实现转发。

简单来说kube-proxy主要在所有的Node节点做如下三件事:

- 如果没有dummy类型虚拟网卡，则创建一个，默认名称为kube-ipvs0;
- 把Kubernetes ClusterIP地址添加到kube-ipvs0，同时添加到ipset中。
- 创建ipvs service，ipvs service地址为ClusterIP以及Cluster Port，ipvs server为所有的Endpoint地址，即Pod IP及端口。
使用ipvs作为kube-proxy后端，不仅提高了转发性能，结合ipset还使iptables规则变得更“干净”清楚，从此再也不怕iptables。








